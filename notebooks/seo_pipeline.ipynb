{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65d6ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d558d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "120da80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html):\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        title = soup.title.string if soup.title else ''\n",
    "        body = ' '.join([p.get_text() for p in soup.find_all(['p', 'article', 'main'])])\n",
    "        return title, body, len(body.split())\n",
    "    except:\n",
    "        return '', '', 0\n",
    "\n",
    "df[['title', 'body_text', 'word_count']] = df['html_content'].apply(\n",
    "    lambda x: pd.Series(extract_text(x))\n",
    ")\n",
    "df.to_csv('../data/extracted_content.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef89995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25f4bf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textstat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "282bf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flesch_reading_ease'] = df['body_text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "df['sentence_count'] = df['body_text'].apply(lambda x: len(x.split('.')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab052f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5)\n",
    "X_keywords = vectorizer.fit_transform(df['body_text'])\n",
    "df['top_keywords'] = [list(vectorizer.get_feature_names_out()) for _ in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f967a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved missing URLs to data/missing_urls.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/extracted_content.csv')\n",
    "missing = df[df['html_content'].isna() | df['body_text'].isna() | (df['body_text'].str.strip() == '')]\n",
    "missing[['url']].to_csv('../data/missing_urls.csv', index=False)\n",
    "print(\"‚úÖ Saved missing URLs to data/missing_urls.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4378dd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error scraping https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips: 403 Client Error: Forbidden for url: https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips\n",
      "‚ö†Ô∏è Error scraping https://www.cloudflare.com/learning/access-management/what-is-ztna/: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/access-management/what-is-ztna/\n",
      "‚ö†Ô∏è Error scraping https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d: 404 Client Error: Not Found for url: https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d\n",
      "‚ö†Ô∏è Error scraping https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/: 404 Client Error: Not Found for url: https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/\n",
      "‚ö†Ô∏è Error scraping https://www.investopedia.com/terms/s/seo.asp: 404 Client Error: Not Found for url: https://www.investopedia.com/terms/s/seo.asp\n",
      "‚ö†Ô∏è Error scraping https://www.reuters.com/technology/artificial-intelligence/: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/artificial-intelligence/\n",
      "‚ö†Ô∏è Error scraping https://www.cnbc.com/artificial-intelligence/: 404 Client Error: Not Found for url: https://www.cnbc.com/artificial-intelligence/\n",
      "‚ö†Ô∏è Error scraping https://www.bbc.com/news/topics/c404v061z99t: 404 Client Error: Not Found for url: https://www.bbc.com/news/topics/c404v061z99t\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def scrape_page(url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        resp = requests.get(url, headers=headers, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        title = soup.title.string if soup.title else ''\n",
    "        body = ' '.join([p.get_text() for p in soup.find_all(['p', 'article', 'main'])])\n",
    "        return title, body, len(body.split())\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error scraping {url}: {e}\")\n",
    "        return '', '', 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if pd.isna(row['body_text']) or row['body_text'].strip() == '':\n",
    "        title, body, wc = scrape_page(row['url'])\n",
    "        df.at[i, 'title'] = title\n",
    "        df.at[i, 'body_text'] = body\n",
    "        df.at[i, 'word_count'] = wc\n",
    "        time.sleep(1.5)  # polite delay between requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "296bef2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, requests, time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64)',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64)',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)',\n",
    "]\n",
    "\n",
    "def scrape_page(url):\n",
    "    for attempt in range(3):  # try up to 3 times\n",
    "        try:\n",
    "            headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
    "            resp = requests.get(url, headers=headers, timeout=15)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "            title = soup.title.string if soup.title else ''\n",
    "            body = ' '.join([p.get_text() for p in soup.find_all(['p', 'article', 'main'])])\n",
    "            return title, body, len(body.split())\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ö†Ô∏è Retry {attempt+1} failed for {url}: {e}\")\n",
    "            time.sleep(2)\n",
    "    print(f\"‚ùå Could not scrape {url} after 3 attempts.\")\n",
    "    return '', '', 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4013118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if title == '' and body == '':\n",
    "    df.at[i, 'body_text'] = 'Page unavailable or restricted'\n",
    "    df.at[i, 'title'] = 'Unavailable'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe788be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7309e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478488de35b34b7bb458f865caa16067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "embeddings = model.encode(df['body_text'].tolist(), show_progress_bar=True)\n",
    "df['embedding'] = embeddings.tolist()\n",
    "df.to_csv('../data/features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c9a673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harsha\\anaconda3\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79f105aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: tokenizers 0.22.1\n",
      "Uninstalling tokenizers-0.22.1:\n",
      "  Successfully uninstalled tokenizers-0.22.1\n",
      "Found existing installation: transformers 4.57.1\n",
      "Uninstalling transformers-4.57.1:\n",
      "  Successfully uninstalled transformers-4.57.1\n",
      "Found existing installation: sentence-transformers 5.1.2\n",
      "Uninstalling sentence-transformers-5.1.2:\n",
      "  Successfully uninstalled sentence-transformers-5.1.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip uninstall -y tokenizers transformers sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc381512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tokenizers<0.24,>=0.22\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from tokenizers<0.24,>=0.22) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (6.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (4.15.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (2025.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (4.67.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (3.19.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.42.1->huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (0.4.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (2025.10.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers<0.24,>=0.22) (3.11)\n",
      "Installing collected packages: tokenizers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "Successfully installed tokenizers-0.22.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install \"tokenizers>=0.22,<0.24\"\n",
    "!{sys.executable} -m pip install \"transformers==4.44.2\"\n",
    "!{sys.executable} -m pip install \"sentence-transformers==3.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60611cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting transformers<5.0.0,>=4.41.0\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.10.23)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\harsha\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.10.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harsha\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
      "Installing collected packages: transformers, sentence-transformers\n",
      "Successfully installed sentence-transformers-5.1.2 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "204068aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Scraping missing content for: https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips\n",
      "‚ö†Ô∏è Attempt 1 failed for https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips: 403 Client Error: Forbidden for url: https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips\n",
      "‚ö†Ô∏è Attempt 2 failed for https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips: 403 Client Error: Forbidden for url: https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips\n",
      "‚ö†Ô∏è Attempt 3 failed for https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips: 403 Client Error: Forbidden for url: https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips\n",
      "‚ùå Could not fetch https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips after 3 retries ‚Äî marking as Unavailable.\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.connectwise.com/blog/phishing-prevention-tips\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.hpe.com/us/en/what-is/sd-wan.html\n",
      "‚ö†Ô∏è Attempt 1 failed for https://www.hpe.com/us/en/what-is/sd-wan.html: HTTPSConnectionPool(host='www.hpe.com', port=443): Read timed out. (read timeout=15)\n",
      "\n",
      "üîÑ Scraping missing content for: https://remotedesktop.google.com/\n",
      "‚ùå Could not fetch https://remotedesktop.google.com/ after 3 retries ‚Äî marking as Unavailable.\n",
      "\n",
      "üîÑ Scraping missing content for: https://support.microsoft.com/en-us/windows/how-to-use-remote-desktop-5fe128d5-8fb1-7a23-3b8a-41e636865e8c\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.cloudflare.com/learning/access-management/what-is-ztna/\n",
      "‚ö†Ô∏è Attempt 1 failed for https://www.cloudflare.com/learning/access-management/what-is-ztna/: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/access-management/what-is-ztna/\n",
      "‚ö†Ô∏è Attempt 2 failed for https://www.cloudflare.com/learning/access-management/what-is-ztna/: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/access-management/what-is-ztna/\n",
      "‚ö†Ô∏è Attempt 3 failed for https://www.cloudflare.com/learning/access-management/what-is-ztna/: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/access-management/what-is-ztna/\n",
      "‚ùå Could not fetch https://www.cloudflare.com/learning/access-management/what-is-ztna/ after 3 retries ‚Äî marking as Unavailable.\n",
      "\n",
      "üîÑ Scraping missing content for: https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d\n",
      "‚ö†Ô∏è Attempt 1 failed for https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d: 404 Client Error: Not Found for url: https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d\n",
      "‚ö†Ô∏è Attempt 2 failed for https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d: 404 Client Error: Not Found for url: https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d\n",
      "‚ö†Ô∏è Attempt 3 failed for https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d: 404 Client Error: Not Found for url: https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d\n",
      "‚ùå Could not fetch https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d after 3 retries ‚Äî marking as Unavailable.\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/\n",
      "‚ö†Ô∏è Attempt 1 failed for https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/: 404 Client Error: Not Found for url: https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/\n",
      "‚ö†Ô∏è Attempt 2 failed for https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/: 404 Client Error: Not Found for url: https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/\n",
      "‚ö†Ô∏è Attempt 3 failed for https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/: 404 Client Error: Not Found for url: https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/\n",
      "‚ùå Could not fetch https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/ after 3 retries ‚Äî marking as Unavailable.\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.investopedia.com/terms/s/seo.asp\n",
      "‚ö†Ô∏è Attempt 1 failed for https://www.investopedia.com/terms/s/seo.asp: 404 Client Error: Not Found for url: https://www.investopedia.com/terms/s/seo.asp\n",
      "‚ö†Ô∏è Attempt 2 failed for https://www.investopedia.com/terms/s/seo.asp: 404 Client Error: Not Found for url: https://www.investopedia.com/terms/s/seo.asp\n",
      "‚ö†Ô∏è Attempt 3 failed for https://www.investopedia.com/terms/s/seo.asp: 404 Client Error: Not Found for url: https://www.investopedia.com/terms/s/seo.asp\n",
      "‚ùå Could not fetch https://www.investopedia.com/terms/s/seo.asp after 3 retries ‚Äî marking as Unavailable.\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.dollardays.com/?srsltid=AfmBOopXjdOu2Kwq6fwYN9FPfB19MorSOf5UyS0EisxFSAzOm8wbl8KF\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.reuters.com/technology/artificial-intelligence/\n",
      "‚ö†Ô∏è Attempt 1 failed for https://www.reuters.com/technology/artificial-intelligence/: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/artificial-intelligence/\n",
      "‚ö†Ô∏è Attempt 2 failed for https://www.reuters.com/technology/artificial-intelligence/: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/artificial-intelligence/\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.cnbc.com/artificial-intelligence/\n",
      "‚ö†Ô∏è Attempt 1 failed for https://www.cnbc.com/artificial-intelligence/: 404 Client Error: Not Found for url: https://www.cnbc.com/artificial-intelligence/\n",
      "‚ö†Ô∏è Attempt 2 failed for https://www.cnbc.com/artificial-intelligence/: 404 Client Error: Not Found for url: https://www.cnbc.com/artificial-intelligence/\n",
      "‚ö†Ô∏è Attempt 3 failed for https://www.cnbc.com/artificial-intelligence/: 404 Client Error: Not Found for url: https://www.cnbc.com/artificial-intelligence/\n",
      "‚ùå Could not fetch https://www.cnbc.com/artificial-intelligence/ after 3 retries ‚Äî marking as Unavailable.\n",
      "\n",
      "üîÑ Scraping missing content for: https://www.bbc.com/news/topics/c404v061z99t\n",
      "‚ö†Ô∏è Attempt 1 failed for https://www.bbc.com/news/topics/c404v061z99t: 404 Client Error: Not Found for url: https://www.bbc.com/news/topics/c404v061z99t\n",
      "‚ö†Ô∏è Attempt 2 failed for https://www.bbc.com/news/topics/c404v061z99t: 404 Client Error: Not Found for url: https://www.bbc.com/news/topics/c404v061z99t\n",
      "‚ö†Ô∏è Attempt 3 failed for https://www.bbc.com/news/topics/c404v061z99t: 404 Client Error: Not Found for url: https://www.bbc.com/news/topics/c404v061z99t\n",
      "‚ùå Could not fetch https://www.bbc.com/news/topics/c404v061z99t after 3 retries ‚Äî marking as Unavailable.\n",
      "\n",
      "‚úÖ Full dataset repaired and saved successfully! All 81 rows retained.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Load existing extracted_content file ---\n",
    "df = pd.read_csv('../data/extracted_content.csv')\n",
    "\n",
    "# --- Define multiple user-agents for rotation (bypasses 403 blocks) ---\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/117.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1'\n",
    "]\n",
    "\n",
    "# --- Scraper function with retries, delays, and graceful fallback ---\n",
    "def scrape_page(url):\n",
    "    for attempt in range(3):  # try 3 times\n",
    "        try:\n",
    "            headers = {'User-Agent': random.choice(USER_AGENTS)}\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.title.string.strip() if soup.title else ''\n",
    "            body = ' '.join([p.get_text(\" \", strip=True) for p in soup.find_all(['p', 'article', 'main'])])\n",
    "            word_count = len(body.split())\n",
    "            if word_count > 0:\n",
    "                return title, body, word_count\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt+1} failed for {url}: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    # If still fails after retries:\n",
    "    print(f\"‚ùå Could not fetch {url} after 3 retries ‚Äî marking as Unavailable.\")\n",
    "    return 'Unavailable', 'Page unavailable or restricted', 0\n",
    "\n",
    "\n",
    "# --- Re-scrape only missing or empty body_text rows ---\n",
    "for i, row in df.iterrows():\n",
    "    if pd.isna(row.get('body_text')) or str(row['body_text']).strip() == '' or row['word_count'] == 0:\n",
    "        print(f\"\\nüîÑ Scraping missing content for: {row['url']}\")\n",
    "        title, body, wc = scrape_page(row['url'])\n",
    "        df.at[i, 'title'] = title\n",
    "        df.at[i, 'body_text'] = body\n",
    "        df.at[i, 'word_count'] = wc\n",
    "        time.sleep(1.5)  # polite delay\n",
    "\n",
    "# --- Final clean-up & save ---\n",
    "if 'html_content' in df.columns:\n",
    "    df = df.drop(columns=['html_content'])\n",
    "\n",
    "df.to_csv('../data/extracted_content.csv', index=False)\n",
    "print(\"\\n‚úÖ Full dataset repaired and saved successfully! All 81 rows retained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c60c4bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 81 rows\n",
      "üîÑ Generating semantic embeddings (this may take ~2‚Äì3 minutes)...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'body_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'body_text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_7524\\4009505492.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"üîÑ Generating semantic embeddings (this may take ~2‚Äì3 minutes)...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all-MiniLM-L6-v2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'body_text'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# === STEP 1: LOAD DATA ===\n",
    "df = pd.read_csv('../data/features.csv')\n",
    "print(f\"‚úÖ Loaded {len(df)} rows\")\n",
    "\n",
    "# === STEP 2: HANDLE MISSING DATA ===\n",
    "df = df.fillna({'flesch_reading_ease': 0, 'word_count': 0, 'sentence_count': 0, 'top_keywords': ''})\n",
    "\n",
    "# === STEP 3: ADD EXTRA FEATURES ===\n",
    "df['keyword_density'] = df['top_keywords'].apply(lambda x: len(str(x).split('|')) if isinstance(x, str) else 0)\n",
    "df['readability_bin'] = pd.cut(df['flesch_reading_ease'], bins=[0, 30, 50, 70, 100],\n",
    "                               labels=[1, 2, 3, 4]).astype(float).fillna(0)\n",
    "\n",
    "# === STEP 4: ADD SEMANTIC EMBEDDINGS ===\n",
    "print(\"üîÑ Generating semantic embeddings (this may take ~2‚Äì3 minutes)...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df['body_text'].astype(str).tolist(), show_progress_bar=True)\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Dimensionality reduction to make it faster\n",
    "pca = PCA(n_components=10, random_state=42)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# === STEP 5: COMBINE ALL FEATURES ===\n",
    "extra_features = df[['word_count', 'sentence_count', 'flesch_reading_ease',\n",
    "                     'keyword_density', 'readability_bin']].to_numpy()\n",
    "X_full = np.hstack([extra_features, reduced_embeddings])\n",
    "\n",
    "# === STEP 6: LABEL QUALITY ===\n",
    "def label_quality(row):\n",
    "    if row['word_count'] > 1500 and 50 <= row['flesch_reading_ease'] <= 70:\n",
    "        return 'High'\n",
    "    elif row['word_count'] < 500 or row['flesch_reading_ease'] < 30:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "df['quality_label'] = df.apply(label_quality, axis=1)\n",
    "y = df['quality_label']\n",
    "\n",
    "# === STEP 7: SPLIT DATA ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.3,\n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "# === STEP 8: SCALE ===\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# === STEP 9: TRAIN HIGH-ACCURACY RANDOM FOREST ===\n",
    "params = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid = GridSearchCV(rf, param_grid=params, cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_rf = grid.best_estimator_\n",
    "print(\"\\nüèÜ Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# === STEP 10: EVALUATE ===\n",
    "y_pred = best_rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"‚úÖ Accuracy: {acc:.3f}\")\n",
    "\n",
    "# === STEP 11: SAVE MODEL ===\n",
    "joblib.dump(best_rf, '../models/quality_model_hybrid.pkl')\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "joblib.dump(pca, '../models/pca.pkl')\n",
    "print(\"üíæ Saved hybrid model (model + scaler + PCA) to ../models/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c93172cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>body_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.cm-alliance.com/cybersecurity-blog</td>\n",
       "      <td>Cyber Security Blog</td>\n",
       "      <td>Cyber Crisis Tabletop Exercise Cyber Security ...</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.varonis.com/blog/cybersecurity-tips</td>\n",
       "      <td>Top 10 Cybersecurity Awareness Tips: How to St...</td>\n",
       "      <td>The #1 Data Security Platform WHERE TO BUY CAP...</td>\n",
       "      <td>5460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.cisecurity.org/insights/blog/11-cy...</td>\n",
       "      <td>11 Cyber Defense Tips to Stay Secure at Work a...</td>\n",
       "      <td>Home Insights Blog Posts 11 Cyber Defense Tips...</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.cisa.gov/topics/cybersecurity-best...</td>\n",
       "      <td>Cybersecurity Best Practices | Cybersecurity a...</td>\n",
       "      <td>An official website of the United States gover...</td>\n",
       "      <td>1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>Unavailable</td>\n",
       "      <td>Page unavailable or restricted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
       "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
       "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
       "3  https://www.cisa.gov/topics/cybersecurity-best...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "\n",
       "                                               title  \\\n",
       "0                                Cyber Security Blog   \n",
       "1  Top 10 Cybersecurity Awareness Tips: How to St...   \n",
       "2  11 Cyber Defense Tips to Stay Secure at Work a...   \n",
       "3  Cybersecurity Best Practices | Cybersecurity a...   \n",
       "4                                        Unavailable   \n",
       "\n",
       "                                           body_text  word_count  \n",
       "0  Cyber Crisis Tabletop Exercise Cyber Security ...         326  \n",
       "1  The #1 Data Security Platform WHERE TO BUY CAP...        5460  \n",
       "2  Home Insights Blog Posts 11 Cyber Defense Tips...        2011  \n",
       "3  An official website of the United States gover...        1438  \n",
       "4                     Page unavailable or restricted           0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/extracted_content.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd28b067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 81 rows from extracted_content.csv\n",
      "üîÑ Generating embeddings ‚Äî this may take 2‚Äì3 minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b14e1840524bf3aefdec8eebadde19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved features.csv successfully!\n",
      "\n",
      "üìä Summary:\n",
      "Average readability: 34.34\n",
      "Average sentence count: 307.83\n",
      "Sample keywords for first 3 rows:\n",
      "                                                 url  \\\n",
      "0     https://www.cm-alliance.com/cybersecurity-blog   \n",
      "1    https://www.varonis.com/blog/cybersecurity-tips   \n",
      "2  https://www.cisecurity.org/insights/blog/11-cy...   \n",
      "\n",
      "                                       top_keywords  \n",
      "0  security|management|training|cybersecurity|cyber  \n",
      "1                sensitive|app|access|security|data  \n",
      "2             device|use|protect|don|authentication  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textstat\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# === STEP 1: LOAD CLEAN EXTRACTED CONTENT ===\n",
    "df = pd.read_csv('../data/extracted_content.csv')\n",
    "print(f\"‚úÖ Loaded {len(df)} rows from extracted_content.csv\")\n",
    "\n",
    "# === STEP 2: BASIC CLEANING ===\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = ' '.join(text.split())  # remove extra spaces\n",
    "    return text.lower()\n",
    "\n",
    "df['clean_text'] = df['body_text'].apply(clean_text)\n",
    "\n",
    "# === STEP 3: FEATURE EXTRACTION ===\n",
    "\n",
    "# Word & sentence counts (already have word_count from earlier)\n",
    "df['sentence_count'] = df['clean_text'].apply(lambda x: len(x.split('.')))\n",
    "df['flesch_reading_ease'] = df['clean_text'].apply(lambda x: textstat.flesch_reading_ease(x) if len(x) > 20 else 0)\n",
    "\n",
    "# === STEP 4: TOP 5 KEYWORDS (TF-IDF) ===\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X_tfidf = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# Get top 5 keywords per document\n",
    "def top_keywords_per_doc(tfidf_vector, feature_names, top_n=5):\n",
    "    sorted_nzs = np.argsort(tfidf_vector.toarray()).flatten()[::-1]\n",
    "    top_features = [feature_names[i] for i in sorted_nzs[:top_n]]\n",
    "    return '|'.join(top_features)\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "df['top_keywords'] = [\n",
    "    '|'.join(feature_names[idx] for idx in np.argsort(row.toarray()).flatten()[-5:])\n",
    "    for row in X_tfidf\n",
    "]\n",
    "\n",
    "# === STEP 5: EMBEDDINGS (for duplicate detection later) ===\n",
    "print(\"üîÑ Generating embeddings ‚Äî this may take 2‚Äì3 minutes...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(df['clean_text'].tolist(), show_progress_bar=True)\n",
    "df['embedding'] = embeddings.tolist()\n",
    "\n",
    "# === STEP 6: SAVE FEATURES ===\n",
    "df_features = df[['url', 'title', 'word_count', 'sentence_count',\n",
    "                  'flesch_reading_ease', 'top_keywords', 'embedding']]\n",
    "\n",
    "df_features.to_csv('../data/features.csv', index=False)\n",
    "print(\"‚úÖ Saved features.csv successfully!\")\n",
    "\n",
    "# === STEP 7: QUICK SUMMARY ===\n",
    "print(\"\\nüìä Summary:\")\n",
    "print(\"Average readability:\", round(df['flesch_reading_ease'].mean(), 2))\n",
    "print(\"Average sentence count:\", round(df['sentence_count'].mean(), 2))\n",
    "print(\"Sample keywords for first 3 rows:\")\n",
    "print(df_features[['url', 'top_keywords']].head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f56c60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 81 rows from features.csv\n",
      "üîÑ Computing cosine similarity between pages...\n",
      "\n",
      "üìä Duplicate Detection Summary\n",
      "Total pages analyzed: 81\n",
      "Duplicate pairs found: 49\n",
      "Thin content pages (<500 words): 23\n",
      "\n",
      "‚úÖ Saved duplicate pairs to ../data/duplicates.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url1</th>\n",
       "      <th>url2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>https://remotedesktop.google.com/</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>https://www.cloudflare.com/learning/access-man...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>https://towardsdatascience.com/machine-learnin...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>https://www.analyticsvidhya.com/blog/2021/09/c...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>https://www.investopedia.com/terms/s/seo.asp</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>https://www.reuters.com/technology/artificial-...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>https://www.cnbc.com/artificial-intelligence/</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://www.qnbtrust.bank/Resources/Learning-C...</td>\n",
       "      <td>https://www.bbc.com/news/topics/c404v061z99t</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.fortinet.com/resources/cyberglossa...</td>\n",
       "      <td>https://www.fortinet.com/resources/cyberglossa...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://www.fortinet.com/resources/cyberglossa...</td>\n",
       "      <td>https://www.fortinet.com/solutions/enterprise-...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url1  \\\n",
       "0  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "1  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "2  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "3  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "4  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "5  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "6  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "7  https://www.qnbtrust.bank/Resources/Learning-C...   \n",
       "8  https://www.fortinet.com/resources/cyberglossa...   \n",
       "9  https://www.fortinet.com/resources/cyberglossa...   \n",
       "\n",
       "                                                url2  similarity  \n",
       "0                  https://remotedesktop.google.com/         1.0  \n",
       "1  https://www.cloudflare.com/learning/access-man...         1.0  \n",
       "2  https://towardsdatascience.com/machine-learnin...         1.0  \n",
       "3  https://www.analyticsvidhya.com/blog/2021/09/c...         1.0  \n",
       "4       https://www.investopedia.com/terms/s/seo.asp         1.0  \n",
       "5  https://www.reuters.com/technology/artificial-...         1.0  \n",
       "6      https://www.cnbc.com/artificial-intelligence/         1.0  \n",
       "7       https://www.bbc.com/news/topics/c404v061z99t         1.0  \n",
       "8  https://www.fortinet.com/resources/cyberglossa...         1.0  \n",
       "9  https://www.fortinet.com/solutions/enterprise-...         1.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === STEP 1: LOAD FEATURES ===\n",
    "df = pd.read_csv('../data/features.csv')\n",
    "print(f\"‚úÖ Loaded {len(df)} rows from features.csv\")\n",
    "\n",
    "# Convert embedding strings back to numeric arrays\n",
    "def parse_embedding(x):\n",
    "    try:\n",
    "        if isinstance(x, str) and x.startswith('['):\n",
    "            return np.fromstring(x.strip('[]'), sep=',')\n",
    "        elif isinstance(x, list) or isinstance(x, np.ndarray):\n",
    "            return np.array(x)\n",
    "        else:\n",
    "            return np.zeros(384)\n",
    "    except:\n",
    "        return np.zeros(384)\n",
    "\n",
    "df['embedding'] = df['embedding'].apply(parse_embedding)\n",
    "\n",
    "# === STEP 2: COSINE SIMILARITY MATRIX ===\n",
    "print(\"üîÑ Computing cosine similarity between pages...\")\n",
    "embeddings_matrix = np.vstack(df['embedding'].to_numpy())\n",
    "similarity_matrix = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# === STEP 3: IDENTIFY DUPLICATES ===\n",
    "threshold = 0.80\n",
    "duplicates = []\n",
    "\n",
    "for i in range(len(similarity_matrix)):\n",
    "    for j in range(i + 1, len(similarity_matrix)):\n",
    "        sim = similarity_matrix[i, j]\n",
    "        if sim > threshold:\n",
    "            duplicates.append([df.iloc[i]['url'], df.iloc[j]['url'], round(sim, 3)])\n",
    "\n",
    "dup_df = pd.DataFrame(duplicates, columns=['url1', 'url2', 'similarity'])\n",
    "dup_df.to_csv('../data/duplicates.csv', index=False)\n",
    "\n",
    "# === STEP 4: FLAG THIN CONTENT PAGES (<500 words) ===\n",
    "df['is_thin'] = df['word_count'] < 500\n",
    "\n",
    "# === STEP 5: PRINT SUMMARY ===\n",
    "print(\"\\nüìä Duplicate Detection Summary\")\n",
    "print(f\"Total pages analyzed: {len(df)}\")\n",
    "print(f\"Duplicate pairs found: {len(dup_df)}\")\n",
    "print(f\"Thin content pages (<500 words): {df['is_thin'].sum()}\")\n",
    "print(\"\\n‚úÖ Saved duplicate pairs to ../data/duplicates.csv\")\n",
    "\n",
    "dup_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd9badfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 81 rows\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "üèÜ Best Parameters: {'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       1.00      0.67      0.80         3\n",
      "         Low       0.87      1.00      0.93        13\n",
      "      Medium       0.88      0.78      0.82         9\n",
      "\n",
      "    accuracy                           0.88        25\n",
      "   macro avg       0.91      0.81      0.85        25\n",
      "weighted avg       0.89      0.88      0.88        25\n",
      "\n",
      "‚úÖ Accuracy: 0.880\n",
      "üíæ Saved hybrid model (model + scaler + PCA) to ../models/\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11e97b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url', 'title', 'word_count', 'sentence_count', 'flesch_reading_ease',\n",
      "       'top_keywords', 'embedding'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/features.csv')\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4a50189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Accuracy: 0.762\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.50      0.33      0.40         3\n",
      "         Low       0.85      1.00      0.92        11\n",
      "      Medium       0.67      0.57      0.62         7\n",
      "\n",
      "    accuracy                           0.76        21\n",
      "   macro avg       0.67      0.63      0.64        21\n",
      "weighted avg       0.74      0.76      0.74        21\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca66cc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 81 rows\n",
      "üîÑ Class distribution after balancing:\n",
      "Low       29\n",
      "Medium    29\n",
      "High      29\n",
      "Name: quality_label, dtype: int64\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "üèÜ Best Parameters: {'bootstrap': True, 'max_depth': 15, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "\n",
      "üìä Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        High       0.90      1.00      0.95         9\n",
      "         Low       1.00      1.00      1.00         9\n",
      "      Medium       1.00      0.89      0.94         9\n",
      "\n",
      "    accuracy                           0.96        27\n",
      "   macro avg       0.97      0.96      0.96        27\n",
      "weighted avg       0.97      0.96      0.96        27\n",
      "\n",
      "‚úÖ Accuracy (single split): 0.963\n",
      "\n",
      "üîÅ Cross-validation accuracies: [0.944 0.778 1.    0.765 0.941]\n",
      "‚úÖ Mean CV accuracy: 0.886\n",
      "üíæ Saved hybrid model (model + scaler + PCA) to ../models/\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# === STEP 1: LOAD DATA ===\n",
    "df = pd.read_csv('../data/features.csv')\n",
    "print(f\"‚úÖ Loaded {len(df)} rows\")\n",
    "\n",
    "# === STEP 2: HANDLE MISSING DATA ===\n",
    "df = df.fillna({\n",
    "    'flesch_reading_ease': 0, 'word_count': 0, 'sentence_count': 0,\n",
    "    'top_keywords': '', 'embedding': ''\n",
    "})\n",
    "\n",
    "# === STEP 3: EXTRA FEATURES ===\n",
    "df['keyword_density'] = df['top_keywords'].apply(\n",
    "    lambda x: len(str(x).split('|')) if isinstance(x, str) else 0\n",
    ")\n",
    "df['readability_bin'] = pd.cut(\n",
    "    df['flesch_reading_ease'], bins=[0, 30, 50, 70, 100],\n",
    "    labels=[1, 2, 3, 4]\n",
    ").astype(float).fillna(0)\n",
    "\n",
    "# === STEP 4: LOAD EMBEDDINGS ===\n",
    "def parse_embedding(x):\n",
    "    try:\n",
    "        return np.array(eval(x))\n",
    "    except:\n",
    "        return np.zeros(384)\n",
    "\n",
    "embeddings = np.vstack(df['embedding'].apply(parse_embedding).values)\n",
    "\n",
    "# === STEP 5: REDUCE DIMENSIONALITY ===\n",
    "pca = PCA(n_components=10, random_state=42)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# === STEP 6: COMBINE FEATURES ===\n",
    "extra_features = df[['word_count', 'sentence_count', 'flesch_reading_ease',\n",
    "                     'keyword_density', 'readability_bin']].to_numpy()\n",
    "X_full = np.hstack([extra_features, reduced_embeddings])\n",
    "\n",
    "# === STEP 7: LABEL QUALITY ===\n",
    "def label_quality(row):\n",
    "    if row['word_count'] > 1500 and 50 <= row['flesch_reading_ease'] <= 70:\n",
    "        return 'High'\n",
    "    elif row['word_count'] < 500 or row['flesch_reading_ease'] < 30:\n",
    "        return 'Low'\n",
    "    else:\n",
    "        return 'Medium'\n",
    "\n",
    "df['quality_label'] = df.apply(label_quality, axis=1)\n",
    "y = df['quality_label']\n",
    "\n",
    "# === STEP 8: BALANCE CLASSES ===\n",
    "low = df[df.quality_label == 'Low']\n",
    "med = df[df.quality_label == 'Medium']\n",
    "high = df[df.quality_label == 'High']\n",
    "\n",
    "low_up = resample(low, replace=True, n_samples=len(med), random_state=42)\n",
    "high_up = resample(high, replace=True, n_samples=len(med), random_state=42)\n",
    "df_balanced = pd.concat([low_up, med, high_up])\n",
    "\n",
    "# Update features and labels after balancing\n",
    "extra_features = df_balanced[['word_count', 'sentence_count', 'flesch_reading_ease',\n",
    "                              'keyword_density', 'readability_bin']].to_numpy()\n",
    "embeddings = np.vstack(df_balanced['embedding'].apply(parse_embedding).values)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "X_full = np.hstack([extra_features, reduced_embeddings])\n",
    "y = df_balanced['quality_label']\n",
    "\n",
    "print(\"üîÑ Class distribution after balancing:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# === STEP 9: SPLIT + SCALE ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_full, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# === STEP 10: RANDOM FOREST + GRIDSEARCH ===\n",
    "params = {\n",
    "    'n_estimators': [300, 400],\n",
    "    'max_depth': [15, 20, None],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "grid = GridSearchCV(rf, param_grid=params, cv=3, n_jobs=-1, verbose=1)\n",
    "grid.fit(X_train, y_train)\n",
    "best_rf = grid.best_estimator_\n",
    "print(\"\\nüèÜ Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# === STEP 11: EVALUATE ===\n",
    "y_pred = best_rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nüìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"‚úÖ Accuracy (single split): {acc:.3f}\")\n",
    "\n",
    "# === STEP 12: 5-FOLD CROSS VALIDATION ===\n",
    "X_scaled = scaler.fit_transform(X_full)\n",
    "cv_scores = cross_val_score(best_rf, X_scaled, y, cv=5)\n",
    "print(\"\\nüîÅ Cross-validation accuracies:\", np.round(cv_scores, 3))\n",
    "print(\"‚úÖ Mean CV accuracy:\", round(cv_scores.mean(), 3))\n",
    "\n",
    "# === STEP 13: SAVE MODEL ===\n",
    "joblib.dump(best_rf, '../models/quality_model_hybrid.pkl')\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "joblib.dump(pca, '../models/pca.pkl')\n",
    "print(\"üíæ Saved hybrid model (model + scaler + PCA) to ../models/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b28dbbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Analyzing: https://www.ibm.com/topics/cybersecurity\n",
      "‚úÖ Predicted Quality: High\n",
      "üìä Probabilities ‚Üí High: 0.51, Medium: 0.45, Low: 0.04\n",
      "ü™û Most similar page: https://www.varonis.com/blog/cybersecurity-tips\n",
      "üîÅ Similarity score: 0.868\n",
      "‚ö†Ô∏è Duplicate or near-duplicate content detected!\n",
      "\n",
      "üîé Analyzing: https://www.varonis.com/blog/cybersecurity-tips\n",
      "‚úÖ Predicted Quality: Medium\n",
      "üìä Probabilities ‚Üí High: 0.40, Medium: 0.55, Low: 0.05\n",
      "ü™û Most similar page: https://www.varonis.com/blog/cybersecurity-tips\n",
      "üîÅ Similarity score: 0.980\n",
      "‚ö†Ô∏è Duplicate or near-duplicate content detected!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# === LOAD TRAINED MODEL ===\n",
    "model = joblib.load('../models/quality_model_hybrid.pkl')\n",
    "scaler = joblib.load('../models/scaler.pkl')\n",
    "pca = joblib.load('../models/pca.pkl')\n",
    "\n",
    "# === LOAD BASE DATA FOR DUPLICATE CHECK ===\n",
    "df_base = pd.read_csv('../data/features.csv')\n",
    "\n",
    "# Parse stored embeddings safely\n",
    "def parse_embedding(x):\n",
    "    try:\n",
    "        return np.array(eval(x))\n",
    "    except:\n",
    "        return np.zeros(384)\n",
    "\n",
    "base_embeddings = np.vstack(df_base['embedding'].apply(parse_embedding).values)\n",
    "base_reduced = pca.transform(base_embeddings)\n",
    "\n",
    "# === UTILITIES ===\n",
    "def clean_text(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def fetch_page_text(url):\n",
    "    \"\"\"Fetches and extracts visible text from a URL.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=10, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = clean_text(soup.get_text(separator=' '))\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# === MAIN ANALYSIS FUNCTION ===\n",
    "def analyze_url(url):\n",
    "    print(f\"\\nüîé Analyzing: {url}\")\n",
    "    text = fetch_page_text(url)\n",
    "    if len(text.strip()) == 0:\n",
    "        print(\"‚ö†Ô∏è Empty or unreadable page.\")\n",
    "        return\n",
    "\n",
    "    # Text stats\n",
    "    word_count = len(text.split())\n",
    "    sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
    "\n",
    "    # Compute readability (simple approximation)\n",
    "    avg_words_per_sentence = word_count / max(sentence_count, 1)\n",
    "    flesch_score = 206.835 - (1.015 * avg_words_per_sentence) - (84.6 * (word_count / max(len(text.split()), 1)))\n",
    "\n",
    "    # Embedding + PCA\n",
    "    embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    new_embed = embed_model.encode([text])\n",
    "    new_reduced = pca.transform(new_embed)\n",
    "\n",
    "    # Assemble features\n",
    "    keyword_density = len(set(text.lower().split())) / max(word_count, 1) * 100\n",
    "    readability_bin = np.digitize(flesch_score, [0, 30, 50, 70, 100])\n",
    "    features = np.array([[word_count, sentence_count, flesch_score,\n",
    "                          keyword_density, readability_bin]])\n",
    "    X_new = np.hstack([features, new_reduced])\n",
    "    X_scaled = scaler.transform(X_new)\n",
    "\n",
    "    # Predict\n",
    "    pred = model.predict(X_scaled)[0]\n",
    "    proba = model.predict_proba(X_scaled)[0]\n",
    "    print(f\"‚úÖ Predicted Quality: {pred}\")\n",
    "    print(f\"üìä Probabilities ‚Üí High: {proba[model.classes_ == 'High'][0]:.2f}, \"\n",
    "          f\"Medium: {proba[model.classes_ == 'Medium'][0]:.2f}, \"\n",
    "          f\"Low: {proba[model.classes_ == 'Low'][0]:.2f}\")\n",
    "\n",
    "    # Duplicate check\n",
    "    dup_sim = cosine_similarity(new_reduced, base_reduced)[0]\n",
    "    top_idx = np.argmax(dup_sim)\n",
    "    print(f\"ü™û Most similar page: {df_base.iloc[top_idx]['url']}\")\n",
    "    print(f\"üîÅ Similarity score: {dup_sim[top_idx]:.3f}\")\n",
    "\n",
    "    if dup_sim[top_idx] > 0.85:\n",
    "        print(\"‚ö†Ô∏è Duplicate or near-duplicate content detected!\")\n",
    "    elif dup_sim[top_idx] > 0.70:\n",
    "        print(\"‚ö†Ô∏è Potential partial overlap.\")\n",
    "    else:\n",
    "        print(\"‚úÖ Unique content.\")\n",
    "analyze_url(\"https://www.ibm.com/topics/cybersecurity\")\n",
    "analyze_url(\"https://www.varonis.com/blog/cybersecurity-tips\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
